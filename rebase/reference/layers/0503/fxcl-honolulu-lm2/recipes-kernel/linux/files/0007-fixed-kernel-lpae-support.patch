From 51c49791feaf2dde7b3cbea98dd4ddf6881ce72a Mon Sep 17 00:00:00 2001
From: Auto Configured <auto.configured>
Date: Fri, 5 Dec 2014 20:18:50 +0900
Subject: [PATCH 07/18] fixed kernel lpae support


Signed-off-by: Auto Configured <auto.configured>
---
 arch/arm/mm/dma-mapping.c    |    8 ++++++++
 arch/arm/mm/ioremap.c        |   31 +++++++++++++++++++++++++++++++
 arch/arm/mm/proc-v7-3level.S |    9 +++++++--
 3 files changed, 46 insertions(+), 2 deletions(-)

diff --git a/arch/arm/mm/dma-mapping.c b/arch/arm/mm/dma-mapping.c
index 89fa305..c1ad5ae 100644
--- a/arch/arm/mm/dma-mapping.c
+++ b/arch/arm/mm/dma-mapping.c
@@ -854,7 +854,11 @@ static void dma_cache_maint_page(struct page *page, unsigned long offset,
 static void __dma_page_cpu_to_dev(struct page *page, unsigned long off,
 	size_t size, enum dma_data_direction dir)
 {
+#if 1	/* ohkuma unsigned long->phys_addr_t change */
+	phys_addr_t paddr;
+#else
 	unsigned long paddr;
+#endif
 
 	dma_cache_maint_page(page, off, size, dir, dmac_map_area);
 
@@ -870,7 +874,11 @@ static void __dma_page_cpu_to_dev(struct page *page, unsigned long off,
 static void __dma_page_dev_to_cpu(struct page *page, unsigned long off,
 	size_t size, enum dma_data_direction dir)
 {
+#if 1   /* ohkuma unsigned long->phys_addr_t change */
+	phys_addr_t paddr;
+#else
 	unsigned long paddr = page_to_phys(page) + off;
+#endif
 
 	/* FIXME: non-speculating: not required */
 	/* don't bother invalidating if DMA to device */
diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 04d9006..db0f5ee 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -103,8 +103,13 @@ void __init add_static_vm_early(struct static_vm *svm)
 	list_add_tail(&svm->list, &curr_svm->list);
 }
 
+#ifdef	CONFIG_ARM_LPAE
+int ioremap_page(unsigned long virt, phys_addr_t phys,
+		 const struct mem_type *mtype)
+#else
 int ioremap_page(unsigned long virt, unsigned long phys,
 		 const struct mem_type *mtype)
+#endif	/* CONFIG_ARM_LPAE yamano */
 {
 	return ioremap_page_range(virt, virt + PAGE_SIZE, phys,
 				  __pgprot(mtype->prot_pte));
@@ -331,11 +336,21 @@ void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
 	return (void __iomem *) (offset + addr);
 }
 
+#ifdef	CONFIG_ARM_LPAE
+void __iomem *__arm_ioremap_caller(phys_addr_t phys_addr, size_t size,
+	unsigned int mtype, void *caller)
+#else
 void __iomem *__arm_ioremap_caller(unsigned long phys_addr, size_t size,
 	unsigned int mtype, void *caller)
+#endif	/* CONFIG_ARM_LPAE yamano */
 {
+#ifdef	CONFIG_ARM__LPAE
+	phys_addr_t last_addr;
+ 	phys_addr_t offset = phys_addr & ~PAGE_MASK;
+#else
 	unsigned long last_addr;
  	unsigned long offset = phys_addr & ~PAGE_MASK;
+#endif	/* CONFIG_ARM_LPAE yamano */
  	unsigned long pfn = __phys_to_pfn(phys_addr);
 
  	/*
@@ -367,12 +382,23 @@ __arm_ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
 }
 EXPORT_SYMBOL(__arm_ioremap_pfn);
 
+#ifdef	CONFIG_ARM_LPAE
+void __iomem * (*arch_ioremap_caller)(phys_addr_t, size_t,
+				      unsigned int, void *) =
+	__arm_ioremap_caller;
+#else
 void __iomem * (*arch_ioremap_caller)(unsigned long, size_t,
 				      unsigned int, void *) =
 	__arm_ioremap_caller;
+#endif	/* CONFIG_ARM_LPAE yamano */
 
+#ifdef	CONFIG_ARM_LPAE
+void __iomem *
+__arm_ioremap(phys_addr_t phys_addr, size_t size, unsigned int mtype)
+#else
 void __iomem *
 __arm_ioremap(unsigned long phys_addr, size_t size, unsigned int mtype)
+#endif	/* CONFIG_ARM_LPAE yamano */
 {
 	return arch_ioremap_caller(phys_addr, size, mtype,
 		__builtin_return_address(0));
@@ -386,8 +412,13 @@ EXPORT_SYMBOL(__arm_ioremap);
  * clocks that would affect normal memory for example. Please see
  * CONFIG_GENERIC_ALLOCATOR for allocating external memory.
  */
+#ifdef	CONFIG_ARM_LPAE
+void __iomem *
+__arm_ioremap_exec(phys_addr_t phys_addr, size_t size, bool cached)
+#else
 void __iomem *
 __arm_ioremap_exec(unsigned long phys_addr, size_t size, bool cached)
+#endif	/* CONFIG_ARM_LPAE yamano */
 {
 	unsigned int mtype;
 
diff --git a/arch/arm/mm/proc-v7-3level.S b/arch/arm/mm/proc-v7-3level.S
index d3095ed..621ea2a 100644
--- a/arch/arm/mm/proc-v7-3level.S
+++ b/arch/arm/mm/proc-v7-3level.S
@@ -75,10 +75,10 @@ ENTRY(cpu_v7_set_pte_ext)
 #ifdef CONFIG_MMU
 	tst	r2, #L_PTE_VALID
 	beq	1f
-	tst	r3, #1 << (57 - 32)		@ L_PTE_NONE
+	tst	r3, #1 << (57 - 32)	@ L_PTE_NONE
 	bicne	r2, #L_PTE_VALID
 	bne	1f
-	tst	r3, #1 << (55 - 32)		@ L_PTE_DIRTY
+	tst	r3, #1 << (55 - 32)	@ L_PTE_DIRTY
 	orreq	r2, #L_PTE_RDONLY
 1:	strd	r2, r3, [r0]
 	ALT_SMP(W(nop))
@@ -129,15 +129,20 @@ ENDPROC(cpu_v7_set_pte_ext)
 	 */
 	orrls	\tmp, \tmp, #TTBR1_SIZE				@ TTBCR.T1SZ
 	mcr	p15, 0, \tmp, c2, c0, 2				@ TTBCR
+	isb							@ fxcl
 	mov	\tmp, \ttbr1, lsr #(32 - ARCH_PGD_SHIFT)	@ upper bits
 	mov	\ttbr1, \ttbr1, lsl #ARCH_PGD_SHIFT		@ lower bits
 	addls	\ttbr1, \ttbr1, #TTBR1_OFFSET
 	mcrr	p15, 1, \ttbr1, \zero, c2			@ load TTBR1
+	isb							@ fxcl 
 	mov	\tmp, \ttbr0, lsr #(32 - ARCH_PGD_SHIFT)	@ upper bits
 	mov	\ttbr0, \ttbr0, lsl #ARCH_PGD_SHIFT		@ lower bits
 	mcrr	p15, 0, \ttbr0, \zero, c2			@ load TTBR0
+	isb							@ fxcl
 	mcrr	p15, 1, \ttbr1, \zero, c2			@ load TTBR1
+	isb							@ fxcl
 	mcrr	p15, 0, \ttbr0, \zero, c2			@ load TTBR0
+	isb
 	.endm
 
 	__CPUINIT
-- 
1.7.1

